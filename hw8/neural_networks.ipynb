{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "# Transform to normalize the data (substract the mean)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandal  T-shirt/top  Dress  Dress\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "w = 28\n",
    "h = 28\n",
    "num_classes = 10\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.input = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(w*h, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This criterion computes the cross entropy loss between input logits and target.\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# We will use Stochastic Gradient Descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 1.0039017132632435\n",
      "  batch 2000 loss: 0.6291074480134994\n",
      "  batch 3000 loss: 0.5656393843200058\n",
      "  batch 4000 loss: 0.5230952619239688\n",
      "  batch 5000 loss: 0.5299863157672807\n",
      "  batch 6000 loss: 0.4830877466970123\n",
      "  batch 7000 loss: 0.47910419949749483\n",
      "  batch 8000 loss: 0.47232004602719097\n",
      "  batch 9000 loss: 0.44711058918247\n",
      "  batch 10000 loss: 0.43985096433013676\n",
      "  batch 11000 loss: 0.4488900303686969\n",
      "  batch 12000 loss: 0.4287363029848784\n",
      "  batch 13000 loss: 0.43530779957212506\n",
      "  batch 14000 loss: 0.4116530294313561\n",
      "  batch 15000 loss: 0.431548576246365\n",
      "LOSS train 0.431548576246365 valid 0.43885111808776855\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.39590466632554305\n",
      "  batch 2000 loss: 0.4107758870207472\n",
      "  batch 3000 loss: 0.3864809963486623\n",
      "  batch 4000 loss: 0.39480259928875605\n",
      "  batch 5000 loss: 0.4009589783435222\n",
      "  batch 6000 loss: 0.39931449970114047\n",
      "  batch 7000 loss: 0.3799007380492985\n",
      "  batch 8000 loss: 0.3983612042386085\n",
      "  batch 9000 loss: 0.3790465884187724\n",
      "  batch 10000 loss: 0.3935937020980054\n",
      "  batch 11000 loss: 0.3628640862745815\n",
      "  batch 12000 loss: 0.3638581472184742\n",
      "  batch 13000 loss: 0.3906712340014055\n",
      "  batch 14000 loss: 0.3370679234797135\n",
      "  batch 15000 loss: 0.3763897399642738\n",
      "LOSS train 0.3763897399642738 valid 0.3915856182575226\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.3564131276004482\n",
      "  batch 2000 loss: 0.3677697312774835\n",
      "  batch 3000 loss: 0.36959232063475067\n",
      "  batch 4000 loss: 0.3471321812607348\n",
      "  batch 5000 loss: 0.350415097047895\n",
      "  batch 6000 loss: 0.35067746868333777\n",
      "  batch 7000 loss: 0.3431716613192111\n",
      "  batch 8000 loss: 0.31880181627877757\n",
      "  batch 9000 loss: 0.3469467259139055\n",
      "  batch 10000 loss: 0.32417684877384456\n",
      "  batch 11000 loss: 0.33729135047458114\n",
      "  batch 12000 loss: 0.3393296814942732\n",
      "  batch 13000 loss: 0.3316690204703482\n",
      "  batch 14000 loss: 0.3367055186193902\n",
      "  batch 15000 loss: 0.350278153066698\n",
      "LOSS train 0.350278153066698 valid 0.37054458260536194\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.29833269512176047\n",
      "  batch 2000 loss: 0.31226026600354817\n",
      "  batch 3000 loss: 0.3321617197582964\n",
      "  batch 4000 loss: 0.3182584378796164\n",
      "  batch 5000 loss: 0.30065975558788344\n",
      "  batch 6000 loss: 0.32126507058920106\n",
      "  batch 7000 loss: 0.3294420201034518\n",
      "  batch 8000 loss: 0.34499084097670857\n",
      "  batch 9000 loss: 0.3121367586645647\n",
      "  batch 10000 loss: 0.3272328175044968\n",
      "  batch 11000 loss: 0.3233029162954772\n",
      "  batch 12000 loss: 0.3329560327891959\n",
      "  batch 13000 loss: 0.30483577324438377\n",
      "  batch 14000 loss: 0.3383699700954603\n",
      "  batch 15000 loss: 0.31948193521099166\n",
      "LOSS train 0.31948193521099166 valid 0.36568742990493774\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.2908002782544354\n",
      "  batch 2000 loss: 0.29006292552978263\n",
      "  batch 3000 loss: 0.2965854825631832\n",
      "  batch 4000 loss: 0.30253009484050564\n",
      "  batch 5000 loss: 0.29931166336976456\n",
      "  batch 6000 loss: 0.28794374254427385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m      3\u001b[0m last_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Here, we use enumerate(training_loader) instead of\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# iter(training_loader) so that we can track the batch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# index and do some intra-epoch reporting\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Every data instance is an input + label pair\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Zero your gradients for every batch!\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:697\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[1;32m    700\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eda/lib/python3.11/site-packages/torch/autograd/profiler.py:738\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_enter_new(\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\n\u001b[1;32m    735\u001b[0m     )\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit:\n\u001b[1;32m    740\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
