{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Generate a predictor vector **X** of length **n = 100** (random vector **X**), as well as a noise vector **ε** of length **n = 100**. Generate a response vector **Y** of length **n = 100** according to the following model:\n",
    "\n",
    "\n",
    "$$ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3 + \\epsilon_i $$\n",
    "\n",
    "\n",
    "where:\n",
    "- $\\beta_0 = 50$\n",
    "- $\\beta_1 = 10$\n",
    "- $\\beta_2 = -20$\n",
    "- $\\beta_3 = 0.1$\n",
    "\n",
    "Perform ridge regression using **X**, **X²**, **X³**, and **X⁴** as predictors. Choose any two different values of **λ** (different from 0 and ∞). With each **λ**, perform ridge regression both **with** and **without** standardizing the predictors. Then, compare the results.\n",
    "\n",
    "**Note:** No built-in functions are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "np.random.seed(6)\n",
    "# define parameters\n",
    "n = 100\n",
    "x = np.random.rand(n)\n",
    "epsilon = np.random.normal(0, 1, n)\n",
    "b0 = 50\n",
    "b1 = 10\n",
    "b2 = -20\n",
    "b3 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the data\n",
    "x_mean = np.mean(x)\n",
    "x_std = np.std(x)\n",
    "x_standardized = (x - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Y\n",
    "y = b0 + b1*x + b2*x**2 + b3*x**3 + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y, lmd):\n",
    "    n, p = X.shape\n",
    "    I = np.eye(p)\n",
    "    beta_hat = np.linalg.inv(X.T @ X + lmd * I) @ X.T @ y\n",
    "    return beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X matrix\n",
    "X = np.column_stack((np.ones(n), x, x**2, x**3, x**4))\n",
    "X_standardized = np.column_stack((np.ones(n), x_standardized, x_standardized**2, x_standardized**3, x_standardized**4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd0, lmd1 = 0.1, 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not standardized & lambda = 0.1, MSE = 0.794941936608605\n",
      "Not standardized & lambda = 0.01, MSE = 0.7387468119546439\n",
      "Standardized & lambda = 0.1, MSE = 0.7420973167905006\n",
      "Standardized & lambda = 0.01, MSE = 0.7329252504766618\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "beta_hat = ridge_regression(X, y, lmd0)\n",
    "# predict and calculate the mean squared error\n",
    "y_hat = X @ beta_hat\n",
    "mse = np.mean((y - y_hat)**2)\n",
    "print(f\"Not standardized & lambda = {lmd0}, MSE = {mse}\")\n",
    "\n",
    "beta_hat1 = ridge_regression(X, y, lmd1)\n",
    "y_hat1 = X @ beta_hat1\n",
    "mse1 = np.mean((y - y_hat1)**2)\n",
    "print(f\"Not standardized & lambda = {lmd1}, MSE = {mse1}\")\n",
    "\n",
    "beta_hat_ = ridge_regression(X_standardized, y, lmd0)\n",
    "y_hat_ = X_standardized @ beta_hat_\n",
    "mse_ = np.mean((y - y_hat_)**2)\n",
    "print(f\"Standardized & lambda = {lmd0}, MSE = {mse_}\")\n",
    "\n",
    "beta_hat1_ = ridge_regression(X_standardized, y, lmd1)\n",
    "y_hat1_ = X_standardized @ beta_hat1_\n",
    "mse1_ = np.mean((y - y_hat1_)**2)\n",
    "print(f\"Standardized & lambda = {lmd1}, MSE = {mse1_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. With both $\\lambda$ 0.1 and 0.01, the mse after standardizing X has dropped, indicating an improved perforamce of ridge regression with standarization.\n",
    "\n",
    "2. By dropping $\\lambda$ from 0.1 to 0.01, the mse decreased, indicating a lower penalty on coefficients with less regularization, introducing smaller bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Use the dataset you generated in Problem 1 and fit the model for the same set of predictors using **Lasso regression**. \n",
    "\n",
    "Choose any two different values of **λ** (different from 0 and ∞). \n",
    "\n",
    "With each **λ**, perform **Lasso regression** without standardizing the predictors. Then perform **Lasso regression** standardizing the predictors. \n",
    "\n",
    "### Questions:\n",
    "- What can you conclude from these experiments?\n",
    "\n",
    "**Note:** No built-in functions are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 5), (100,), (100, 5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, X_standardized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_reg:\n",
    "    def __init__(self, lmd, tol=1e-6, max_iter=1000):\n",
    "        self.lmd = lmd\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.coef_ = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        n, p = X.shape\n",
    "        b = np.zeros(p)\n",
    "        b_old = np.zeros(p)\n",
    "\n",
    "        X_t_X = X.T @ X\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            for j in range(p):\n",
    "                # compute the partial residual\n",
    "                residual = Y - X @ b + X[:, j] * b[j]\n",
    "\n",
    "                # update coefficient using soft-thresholding\n",
    "                rho = X[:, j].T @ residual\n",
    "                b[j] = self._soft_threshold(rho / X_t_X[j, j], self.lmd)\n",
    "\n",
    "            # early stop if converge\n",
    "            if np.linalg.norm(b - b_old, ord=2) < self.tol:\n",
    "                break\n",
    "\n",
    "            b_old = b.copy()\n",
    "\n",
    "        self.coef_ = b\n",
    "\n",
    "    def _soft_threshold(self, rho, lmd):\n",
    "        if rho > lmd:\n",
    "            return rho - lmd\n",
    "        elif rho < -lmd:\n",
    "            return rho + lmd\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.coef_\n",
    "    \n",
    "    def cal_mse(self, X, Y):\n",
    "        Y_hat = self.predict(X)\n",
    "        return np.mean((Y - Y_hat)**2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd0, lmd1 = 0.1, 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "x_train = x[:80]\n",
    "x_test = x[80:]\n",
    "y_train = y[:80]\n",
    "y_test = y[80:]\n",
    "# standardizing the data\n",
    "x_train_standardized = (x_train - np.mean(x_train)) / np.std(x_train)\n",
    "x_test_standardized = (x_test - np.mean(x_train)) / np.std(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.column_stack((np.ones(80), x_train, x_train**2, x_train**3, x_train**4))\n",
    "X_train_standardized = np.column_stack((np.ones(80), x_train_standardized, x_train_standardized**2, x_train_standardized**3, x_train_standardized**4))\n",
    "X_test = np.column_stack((np.ones(20), x_test, x_test**2, x_test**3, x_test**4))\n",
    "X_test_standardized = np.column_stack((np.ones(20), x_test_standardized, x_test_standardized**2, x_test_standardized**3, x_test_standardized**4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not standardized & lambda = 0.1, Train MSE = 0.9795324278919608, Test MSE = 0.36428484774834435\n",
      "Not standardized & lambda = 0.01, Train MSE = 0.8430831966174044, Test MSE = 0.4200338173817821\n",
      "Standardized & lambda = 0.1, Train MSE = 0.9319593820315231, Test MSE = 0.4162437810188807\n",
      "Standardized & lambda = 0.01, Train MSE = 0.7911449728278032, Test MSE = 0.512160403542953\n"
     ]
    }
   ],
   "source": [
    "# case without standardizing the data\n",
    "lasso0 = lasso_reg(lmd0)\n",
    "lasso0.fit(X_train, y_train)\n",
    "# calcualte train and test MSE\n",
    "mse_train0 = lasso0.cal_mse(X_train, y_train)\n",
    "mse_test0 = lasso0.cal_mse(X_test, y_test)\n",
    "print(f\"Not standardized & lambda = {lmd0}, Train MSE = {mse_train0}, Test MSE = {mse_test0}\")\n",
    "\n",
    "lasso1 = lasso_reg(lmd1)\n",
    "lasso1.fit(X_train, y_train)\n",
    "# calcualte train and test MSE\n",
    "mse_train1 = lasso1.cal_mse(X_train, y_train)\n",
    "mse_test1 = lasso1.cal_mse(X_test, y_test)\n",
    "print(f\"Not standardized & lambda = {lmd1}, Train MSE = {mse_train1}, Test MSE = {mse_test1}\")\n",
    "\n",
    "# case with standardizing the data\n",
    "lasso0_ = lasso_reg(lmd0)\n",
    "lasso0_.fit(X_train_standardized, y_train)\n",
    "# calcualte train and test MSE\n",
    "mse_train0_ = lasso0_.cal_mse(X_train_standardized, y_train)\n",
    "mse_test0_ = lasso0_.cal_mse(X_test_standardized, y_test)\n",
    "print(f\"Standardized & lambda = {lmd0}, Train MSE = {mse_train0_}, Test MSE = {mse_test0_}\")\n",
    "\n",
    "lasso1_ = lasso_reg(lmd1)\n",
    "lasso1_.fit(X_train_standardized, y_train)\n",
    "# calcualte train and test MSE\n",
    "mse_train1_ = lasso1_.cal_mse(X_train_standardized, y_train)\n",
    "mse_test1_ = lasso1_.cal_mse(X_test_standardized, y_test)\n",
    "print(f\"Standardized & lambda = {lmd1}, Train MSE = {mse_train1_}, Test MSE = {mse_test1_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. With both $\\lambda$ 0.1 and 0.01, either test and train set, the mse after standardizing X has dropped, indicating an improved perforamce of lasso regression with standarization.\n",
    "\n",
    "2. By dropping $\\lambda$ from 0.1 to 0.01, the mse deviate differnetly in tain and test sets. MSE would decrease in train set and increase in test set when $\\lambda$ drops from 0.1 to 0.01. Indicating decrease $\\lambda$ would increase the overfitting problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Personality_Coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
