{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Generate a predictor vector **X** of length **n = 100** (random vector **X**), as well as a noise vector **ε** of length **n = 100**. Generate a response vector **Y** of length **n = 100** according to the following model:\n",
    "\n",
    "\n",
    "$$ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3 + \\epsilon_i $$\n",
    "\n",
    "\n",
    "where:\n",
    "- $\\beta_0 = 50$\n",
    "- $\\beta_1 = 10$\n",
    "- $\\beta_2 = -20$\n",
    "- $\\beta_3 = 0.1$\n",
    "\n",
    "Perform ridge regression using **X**, **X²**, **X³**, and **X⁴** as predictors. Choose any two different values of **λ** (different from 0 and ∞). With each **λ**, perform ridge regression both **with** and **without** standardizing the predictors. Then, compare the results.\n",
    "\n",
    "**Note:** No built-in functions are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "np.random.seed(403)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "n = 100\n",
    "x = np.random.rand(n)\n",
    "epsilon = np.random.normal(0, 1, n)\n",
    "b0 = 50\n",
    "b1 = 10\n",
    "b2 = -20\n",
    "b3 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the data\n",
    "x_mean = np.mean(x)\n",
    "x_std = np.std(x)\n",
    "x_standardized = (x - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Y\n",
    "Y = b0 + b1*x + b2*x**2 + b3*x**3 + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, Y, lmd):\n",
    "    n, p = X.shape\n",
    "    I = np.eye(p)\n",
    "    beta_hat = np.linalg.inv(X.T @ X + lmd * I) @ X.T @ Y\n",
    "    return beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse with lambda = 0.1 without standardizing the data: 1.0939062762083729\n",
      "mse with lambda = 0.1 with standardizing the data: 1.0740449019290557\n"
     ]
    }
   ],
   "source": [
    "# not standardizing the data\n",
    "lmd0 = 0.1\n",
    "b_ridge0 = ridge_regression(np.column_stack((np.ones(n), x, x**2, x**3)), Y, lmd0)\n",
    "# predict using the estimated coefficients\n",
    "Y_hat0 = np.column_stack((np.ones(n), x, x**2, x**3)) @ b_ridge0\n",
    "# calculate the mse\n",
    "mse0 = np.mean((Y - Y_hat0)**2)\n",
    "print(f\"mse with lambda = {lmd0} without standardizing the data: {mse0}\")\n",
    "\n",
    "# standardizing the data\n",
    "b_ridge0_ = ridge_regression(np.column_stack((np.ones(n), x_standardized, x_standardized**2, x_standardized**3)), Y, lmd0)\n",
    "# predict using the estimated coefficients\n",
    "Y_hat0_ = np.column_stack((np.ones(n), x_standardized, x_standardized**2, x_standardized**3)) @ b_ridge0_\n",
    "# calculate the mse\n",
    "mse0_ = np.mean((Y - Y_hat0_)**2)\n",
    "print(f\"mse with lambda = {lmd0} with standardizing the data: {mse0_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse with lambda = 0.01 without standardizing the data: 1.0742068107622686\n",
      "mse with lambda = 0.01 with standardizing the data: 1.068984594996808\n"
     ]
    }
   ],
   "source": [
    "lmd1 = 0.01\n",
    "# not standardizing the data\n",
    "b_ridge1 = ridge_regression(np.column_stack((np.ones(n), x, x**2, x**3)), Y, lmd1)\n",
    "# predict using the estimated coefficients\n",
    "Y_hat1 = np.column_stack((np.ones(n), x, x**2, x**3)) @ b_ridge1\n",
    "# calculate the mse\n",
    "mse1 = np.mean((Y - Y_hat1)**2)\n",
    "print(f\"mse with lambda = {lmd1} without standardizing the data: {mse1}\")\n",
    "\n",
    "# standardizing the data\n",
    "b_ridge1_ = ridge_regression(np.column_stack((np.ones(n), x_standardized, x_standardized**2, x_standardized**3)), Y, lmd1)\n",
    "# predict using the estimated coefficients\n",
    "Y_hat1_ = np.column_stack((np.ones(n), x_standardized, x_standardized**2, x_standardized**3)) @ b_ridge1_\n",
    "# calculate the mse\n",
    "mse1_ = np.mean((Y - Y_hat1_)**2)\n",
    "print(f\"mse with lambda = {lmd1} with standardizing the data: {mse1_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. With both $\\lambda$ 0.1 and 0.01, the mse after standardizing X has dropped, indicating an improved perforamce of ridge regression with standarization.\n",
    "\n",
    "2. By dropping $\\lambda$ from 0.1 to 0.01, the mse decreased, indicating a lower penalty on coefficients with less regularization, introducing smaller bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Use the dataset you generated in Problem 1 and fit the model for the same set of predictors using **Lasso regression**. \n",
    "\n",
    "Choose any two different values of **λ** (different from 0 and ∞). \n",
    "\n",
    "With each **λ**, perform **Lasso regression** without standardizing the predictors. Then perform **Lasso regression** standardizing the predictors. \n",
    "\n",
    "### Questions:\n",
    "- What can you conclude from these experiments?\n",
    "\n",
    "**Note:** No built-in functions are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (100,), (100,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, Y.shape, x_standardized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1, 0.01)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmd0, lmd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_reg:\n",
    "    def __init__(self, lmd, tol=1e-4, max_iter=1000):\n",
    "        self.lmd = lmd\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.coef_ = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        n, p = X.shape\n",
    "        b = np.zeros(p)\n",
    "        b_old = np.zeros(p)\n",
    "\n",
    "        X_t_X = X.T @ X\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            for j in range(p):\n",
    "                # compute the partial residual\n",
    "                residual = Y - X @ b + X[:, j] * b[j]\n",
    "\n",
    "                # update coefficient using soft-thresholding\n",
    "                rho = X[:, j].T @ residual\n",
    "                b[j] = self._soft_threshold(rho / X_t_X[j, j], self.lmd)\n",
    "\n",
    "            # early stop if converge\n",
    "            if np.linalg.norm(b - b_old, ord=2) < self.tol:\n",
    "                break\n",
    "\n",
    "            b_old = b.copy()\n",
    "\n",
    "        self.coef_ = b\n",
    "\n",
    "    def _soft_threshold(self, rho, lmd):\n",
    "        if rho > lmd:\n",
    "            return rho - lmd\n",
    "        elif rho < -lmd:\n",
    "            return rho + lmd\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.coef_\n",
    "    \n",
    "    def cal_mse(self, X, Y):\n",
    "        Y_hat = self.predict(X)\n",
    "        return np.mean((Y - Y_hat)**2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse with lambda = 0.1 without standardizing the data: 1.1508305732256225\n",
      "mse with lambda = 0.01 without standardizing the data: 1.0871402994665913\n"
     ]
    }
   ],
   "source": [
    "# case without standardizing the data\n",
    "lasso0 = lasso_reg(lmd0)\n",
    "lasso0.fit(np.column_stack((np.ones(n), x, x**2, x**3)), Y)\n",
    "# calculate the mse\n",
    "mse_lasso0 = lasso0.cal_mse(np.column_stack((np.ones(n), x, x**2, x**3)), Y)\n",
    "print(f\"mse with lambda = {lmd0} without standardizing the data: {mse_lasso0}\")\n",
    "\n",
    "lasso1 = lasso_reg(lmd1)\n",
    "lasso1.fit(np.column_stack((np.ones(n), x, x**2, x**3)), Y)\n",
    "# calculate the mse\n",
    "mse_lasso1 = lasso1.cal_mse(np.column_stack((np.ones(n), x, x**2, x**3)), Y)\n",
    "print(f\"mse with lambda = {lmd1} without standardizing the data: {mse_lasso1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse with lambda = 0.1 with standardizing the data: 1.1755404017481759\n",
      "mse with lambda = 0.01 with standardizing the data: 1.070824122036093\n"
     ]
    }
   ],
   "source": [
    "# case with standardizing the data\n",
    "lasso0_ = lasso_reg(lmd0)\n",
    "lasso0_.fit(np.column_stack((np.ones(n), x_standardized, x_standardized**2, x_standardized**3)), Y)\n",
    "# calculate the mse\n",
    "mse_lasso0_ = lasso0_.cal_mse(np.column_stack((np.ones(n), x_standardized, x_standardized**2, x_standardized**3)), Y)\n",
    "print(f\"mse with lambda = {lmd0} with standardizing the data: {mse_lasso0_}\")\n",
    "\n",
    "lasso1_ = lasso_reg(lmd1)\n",
    "lasso1_.fit(np.column_stack((np.ones(n), x_standardized, x_standardized**2, x_standardized**3)), Y)\n",
    "# calculate the mse\n",
    "mse_lasso1_ = lasso1_.cal_mse(np.column_stack((np.ones(n), x_standardized, x_standardized**2, x_standardized**3)), Y)\n",
    "print(f\"mse with lambda = {lmd1} with standardizing the data: {mse_lasso1_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since unclear the effect of changing lambda, use train-test split to evaluate the model\n",
    "x_train = x[:80]\n",
    "x_test = x[80:]\n",
    "y_train = Y[:80]\n",
    "y_test = Y[80:]\n",
    "# standardizing the data\n",
    "x_train_standardized = (x_train - np.mean(x_train)) / np.std(x_train)\n",
    "x_test_standardized = (x_test - np.mean(x_train)) / np.std(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse with lambda = 0.1 without standardizing the data: 1.2612498747532779\n",
      "test mse with lambda = 0.1 without standardizing the data: 0.6818476146548968\n",
      "train mse with lambda = 0.01 without standardizing the data: 1.1930347305758442\n",
      "test mse with lambda = 0.01 without standardizing the data: 0.6829818446647704\n"
     ]
    }
   ],
   "source": [
    "# case without standardizing the data\n",
    "lasso0 = lasso_reg(lmd0)\n",
    "lasso0.fit(np.column_stack((np.ones(80), x_train, x_train**2, x_train**3)), y_train)\n",
    "# calculate train mse\n",
    "mse_train_lasso0 = lasso0.cal_mse(np.column_stack((np.ones(80), x_train, x_train**2, x_train**3)), y_train)\n",
    "# calculate test mse\n",
    "mse_test_lasso0 = lasso0.cal_mse(np.column_stack((np.ones(20), x_test, x_test**2, x_test**3)), y_test)\n",
    "print(f\"train mse with lambda = {lmd0} without standardizing the data: {mse_train_lasso0}\")\n",
    "print(f\"test mse with lambda = {lmd0} without standardizing the data: {mse_test_lasso0}\")\n",
    "\n",
    "lasso1 = lasso_reg(lmd1)\n",
    "lasso1.fit(np.column_stack((np.ones(80), x_train, x_train**2, x_train**3)), y_train)\n",
    "# calculate train mse\n",
    "mse_train_lasso1 = lasso1.cal_mse(np.column_stack((np.ones(80), x_train, x_train**2, x_train**3)), y_train)\n",
    "# calculate test mse\n",
    "mse_test_lasso1 = lasso1.cal_mse(np.column_stack((np.ones(20), x_test, x_test**2, x_test**3)), y_test)\n",
    "print(f\"train mse with lambda = {lmd1} without standardizing the data: {mse_train_lasso1}\")\n",
    "print(f\"test mse with lambda = {lmd1} without standardizing the data: {mse_test_lasso1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse with lambda = 0.1 with standardizing the data: 1.2858833371163871\n",
      "test mse with lambda = 0.1 with standardizing the data: 0.6007472024541338\n",
      "train mse with lambda = 0.01 with standardizing the data: 1.1760703588835029\n",
      "test mse with lambda = 0.01 with standardizing the data: 0.6446689662376079\n"
     ]
    }
   ],
   "source": [
    "# case with standardizing the data\n",
    "lasso0_ = lasso_reg(lmd0)\n",
    "lasso0_.fit(np.column_stack((np.ones(80), x_train_standardized, x_train_standardized**2, x_train_standardized**3)), y_train)\n",
    "# calculate train mse\n",
    "mse_train_lasso0_ = lasso0_.cal_mse(np.column_stack((np.ones(80), x_train_standardized, x_train_standardized**2, x_train_standardized**3)), y_train)\n",
    "# calculate test mse\n",
    "mse_test_lasso0_ = lasso0_.cal_mse(np.column_stack((np.ones(20), x_test_standardized, x_test_standardized**2, x_test_standardized**3)), y_test)\n",
    "print(f\"train mse with lambda = {lmd0} with standardizing the data: {mse_train_lasso0_}\")\n",
    "print(f\"test mse with lambda = {lmd0} with standardizing the data: {mse_test_lasso0_}\")\n",
    "\n",
    "lasso1_ = lasso_reg(lmd1)\n",
    "lasso1_.fit(np.column_stack((np.ones(80), x_train_standardized, x_train_standardized**2, x_train_standardized**3)), y_train)\n",
    "# calculate train mse\n",
    "mse_train_lasso1_ = lasso1_.cal_mse(np.column_stack((np.ones(80), x_train_standardized, x_train_standardized**2, x_train_standardized**3)), y_train)\n",
    "# calculate test mse\n",
    "mse_test_lasso1_ = lasso1_.cal_mse(np.column_stack((np.ones(20), x_test_standardized, x_test_standardized**2, x_test_standardized**3)), y_test)\n",
    "print(f\"train mse with lambda = {lmd1} with standardizing the data: {mse_train_lasso1_}\")\n",
    "print(f\"test mse with lambda = {lmd1} with standardizing the data: {mse_test_lasso1_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. With both $\\lambda$ 0.1 and 0.01, either test and train set, the mse after standardizing X has dropped, indicating an improved perforamce of lasso regression with standarization.\n",
    "\n",
    "2. By dropping $\\lambda$ from 0.1 to 0.01, the mse deviate differnetly in tain and test sets. MSE would decrease in train set and increase in test set when $\\lambda$ drops from 0.1 to 0.01. Indicating decrease $\\lambda$ would increase the overfitting problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Personality_Coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
